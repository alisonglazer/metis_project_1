{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Mapping tech hubs and combining stations data with turnstile data to plot traffic on a map of NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from geopy.distance import great_circle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using geopy to make a column for tech hubs in stations dataset (coordinates found via google maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "midtownCenter = (40.754925, -73.984063)\n",
    "fidiCenter = (40.709761, -74.006453)\n",
    "flatCenter = (40.741568, -73.989095)\n",
    "dumboCenter = (40.701602, -73.985842)\n",
    "chelseaCenter = (40.746531, -73.997195)\n",
    "sohoCenter = (40.724173, -74.000683)\n",
    "boroughDict = {}\n",
    "boroughDict[\"midtown\"] = midtownCenter\n",
    "boroughDict[\"fidi\"] = fidiCenter\n",
    "boroughDict[\"flatiron\"] = flatCenter\n",
    "boroughDict[\"dumbo\"] = dumboCenter\n",
    "boroughDict[\"chelsea\"] = chelseaCenter\n",
    "boroughDict['soho'] = sohoCenter\n",
    "\n",
    "#This function returns the string representation of the likely borough, given a set of latitude/longitude coordinates\n",
    "#If the distance to the borough center is too far away from the closest borough, we assume that the location\n",
    "#is outside of a tech hub\n",
    "def get_closest_borough(latitude,longitude,max_dist = 2):\n",
    "    global boroughDict\n",
    "    borough_distances = {borough:great_circle(boroughDict[borough],(latitude,longitude)).miles for borough in boroughDict}\n",
    "    min_borough = min(borough_distances, key=borough_distances.get)\n",
    "    if borough_distances[min_borough] < max_dist:\n",
    "        return min_borough \n",
    "    else:\n",
    "        return \"outside_tech_hub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'midtown'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking function\n",
    "get_closest_borough(40.754925, -73.984063)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Station ID', 'Division', 'Stop Name', 'Borough', 'GTFS Latitude',\n",
       "       'GTFS Longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping tech hubs onto stations dateset and grabbing tech stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = station_df['GTFS Latitude']\n",
    "longitude = station_df['GTFS Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df['tech_borough'] = list(map(get_closest_borough, latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_stations = station_df.loc[station_df['tech_borough']!= 'outside_tech_hub', 'Stop Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stations dataset now holds coordinates and a column for which tech hub (or outside of tech hub) these coordinates are located in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commence effort to combine stations dataset with turnstile dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data/clean_df.pickle','rb') as read_file:\n",
    "    clean_df = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/station_df.pickle','rb') as read_file:\n",
    "    station_df = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>entries</th>\n",
       "      <th>exits</th>\n",
       "      <th>date_time</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>turnstile_id</th>\n",
       "      <th>exit_counts</th>\n",
       "      <th>entry_counts</th>\n",
       "      <th>total_traffic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>6470223</td>\n",
       "      <td>2190140</td>\n",
       "      <td>2018-01-06 07:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>6470256</td>\n",
       "      <td>2190229</td>\n",
       "      <td>2018-01-06 11:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>6470379</td>\n",
       "      <td>2190299</td>\n",
       "      <td>2018-01-06 15:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>6470665</td>\n",
       "      <td>2190366</td>\n",
       "      <td>2018-01-06 19:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>6470809</td>\n",
       "      <td>2190398</td>\n",
       "      <td>2018-01-06 23:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station  entries    exits           date_time  weekday  hour  year  \\\n",
       "0   59 ST  6470223  2190140 2018-01-06 07:00:00        5     7  2018   \n",
       "1   59 ST  6470256  2190229 2018-01-06 11:00:00        5    11  2018   \n",
       "2   59 ST  6470379  2190299 2018-01-06 15:00:00        5    15  2018   \n",
       "3   59 ST  6470665  2190366 2018-01-06 19:00:00        5    19  2018   \n",
       "4   59 ST  6470809  2190398 2018-01-06 23:00:00        5    23  2018   \n",
       "\n",
       "   turnstile_id  exit_counts  entry_counts  total_traffic  \n",
       "0             0         17.0           7.0           24.0  \n",
       "1             0         89.0          33.0          122.0  \n",
       "2             0         70.0         123.0          193.0  \n",
       "3             0         67.0         286.0          353.0  \n",
       "4             0         32.0         144.0          176.0  "
      ]
     },
     "execution_count": 925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column cleaning: Strategy 1\n",
    "#### The only column which could be used to merge these datasets is a text column. And it is messy. \n",
    "#### Strategy 1 involves regex in attempts to bring these columns closer together. Trying to remove -, /, spaces, capitalize, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_name = station_df['Stop Name']\n",
    "station_name = clean_df['station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_name = [x.upper() for x in stop_name]\n",
    "station_name = [x.upper() for x in station_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_name = [re.sub(r'AVE','AV', x) for x in stop_name]\n",
    "station_name = [re.sub(r'AVE','AV',x) for x in station_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_name = [re.sub(r'AVS','AV', x) for x in stop_name]\n",
    "station_name = [re.sub(r'AVS','AV',x) for x in station_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_name = [re.sub(r'/',' ',x) for x in stop_name]\n",
    "station_name = [re.sub('/',' ',x) for x in station_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_name = [re.sub(r'-', '',x) for x in stop_name]\n",
    "station_name = [re.sub('-', '',x) for x in station_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_name = [x.strip() for x in stop_name]\n",
    "station_name = [x.strip() for x in station_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_name = [re.sub(r' +',' ', x) for x in stop_name]\n",
    "station_name = [re.sub(r' +',' ', x) for x in station_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strips all spaces. Not super practical. Not readable.\n",
    "#tech_stations = sorted([re.sub(r\"\\s+\", \"\", x) for x in tech_stations])\n",
    "#unique_stations_turn = sorted([re.sub(r\"\\s+\",\"\",x) for x in unique_stations_turn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how many columns line up after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1342325"
      ]
     },
     "execution_count": 979,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_stations = []\n",
    "for station in station_name:\n",
    "    if station in stop_name:\n",
    "        combined_stations.append(station)\n",
    "len(combined_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Columns are better. We now capture greater than 50% in common, but still not enough. Pickling to not have to redo these cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cleaner_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(clean_df, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data/cleaner_df.pickle','rb') as read_file:\n",
    "    clean_df = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/clean_station_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(station_df, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/clean_station_df.pickle','rb') as read_file:\n",
    "    station_df = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: Brittany's Insanity\n",
    "#### Brittany's Insanity: Going through and manually filtering column data to merge stations with turnstiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deleting 14th st from stations\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == '14TH STREET'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == '14TH STREET'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deleting 168 Washington from turnstile\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == '34 STHUDSON YD'].index, axis = 0)\n",
    "## Deleting 34 Hudson from station\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == '168 ST WASHINGTON HTS'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '46 ST BLISS ST' from turnstile\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == '46 ST BLISS ST'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4AV9 ST from turnstile\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == '4AV9 ST'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '72 ST2 AV' from turnstile\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == '72 ST2 AV'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '86 ST2 AV','96 ST2 AV','9TH STREET' from turnstile '9 ST' from station\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == '86 ST2 AV'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == '96 ST2 AV'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == '9TH STREET'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == '9 ST'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'ANNADALE' AND 'ARTHUR KILL' from station\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'ANNADALE'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'ARTHUR KILL'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"B'WAYLAFAYETTE\" and 'BAY TERRACE'\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'BAY TERRACE'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == \"B'WAYLAFAYETTE\"].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAY RIDGE AV': 'BAY RIDGE 95 ST',\n",
    " #'BAY RIDGE95 ST': 'BAY RIDGE AV',\n",
    " #'BEDFORD PK BLVD': 'BEDFORD NOSTRAND AV',\n",
    " #'BEDFORDNOSTRAN': 'BEDFORD PARK BLVD',\n",
    "    #'BEDFORD PARK BLVD LEHMAN COLLEGE' from stations\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'BAY RIDGE AV'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'BAY RIDGE AV'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'BAY RIDGE 95 ST'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'BAY RIDGE95 ST'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'BEDFORD NOSTRAND AV'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'BEDFORD PK BLVD'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'BEDFORD PARK BLVD'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'BEDFORDNOSTRAN'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'BEDFORD PARK BLVD LEHMAN COLLEGE'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'BROADWAYLAFAYETTE ST' from stations\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'BROADWAYLAFAYETTE ST'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CATHEDRAL PKWY (110 ST)' from stations\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'CATHEDRAL PKWY (110 ST)'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CITY BUS' from turnstile 'CLIFTON' from stations\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'CITY BUS'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'CLIFTON'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'DONGAN HILLS' from stations\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'DONGAN HILLS'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ELTINGVILLE' from stations\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'ELTINGVILLE'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'GROVE STREET'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'GRANT CITY'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'GRASMERE'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'GREAT KILLS'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'HARRISON' from turnstil\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'HARRISON'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'HOYT SCHERMERHORN STS' from stations\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'HOYT SCHERMERHORN STS'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'JACKSON HTS ROOSEVELT AV' from stations\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'JACKSON HTS ROOSEVELT AV'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'JOURNAL SQUARE' 'JFK JAMAICA CT1' from turnstiles\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'JOURNAL SQUARE'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'JFK JAMAICA CT1'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LACKAWANNA' AND 'LEXINGTON AV 53 ST' from turnstile and station\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'LACKAWANNA'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'LEXINGTON AV 53 ST'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'MIDDLE VILLAGE METROPOLITAN AV' from station\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'MIDDLE VILLAGE METROPOLITAN AV'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEWARK BM BW','NEWARK C', 'NEWARK HM HE', 'NEWARK HW BMEBE' in turnstiles\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'NEWARK BM BW'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'NEWARK C'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'NEWARK HM HE'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'NEWARK HW BMEBE'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'OAKWOOD HEIGHTS' from stations\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'OAKWOOD HEIGHTS'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'PATH NEW WTC', 'PATH WTC 2', 'PAVONIA NEWPORT' from turnstiles 'PLEASANT PLAINS' from stations\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'PATH NEW WTC'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'PATH WTC 2'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'PAVONIA NEWPORT'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'PLEASANT PLAINS'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"PRINCE'S BAY\" from stations, 'RITMANHATTAN' FROM TURNSTILE\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'RITMANHATTAN'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == \"PRINCE'S BAY\"].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAPLETON' from stations and 'SUTPHINARCHER' from turnstile\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'SUTPHINARCHER'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == \"STAPLETON\"].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'SUTPHIN BLVD ARCHER AV JFK AIRPORT' from station THIRTY ST','THIRTY THIRD ST' from turnstiles\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'THIRTY ST'].index, axis = 0)\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'THIRTY THIRD ST'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'SUTPHIN BLVD ARCHER AV JFK AIRPORT'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTTENVILLE','UNION SQ 14 ST' from station, 'TWENTY THIRD ST' from turnstiles\n",
    "clean_df = clean_df.drop(clean_df[clean_df['station'] == 'TWENTY THIRD ST'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'TOTTENVILLE'].index, axis = 0)\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'UNION SQ 14 ST'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'WOODSIDE 61 ST' from station\n",
    "station_df = station_df.drop(station_df[station_df['Stop Name'] == 'WOODSIDE 61 ST'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how many columns now line up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1342325"
      ]
     },
     "execution_count": 979,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_stations = []\n",
    "for station in station_name:\n",
    "    if station in stop_name:\n",
    "        combined_stations.append(station)\n",
    "len(combined_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Check Steps Below\n",
    "\n",
    "I print the length of each new column, compare them side by side, and fix issues above (by deleting column items) and below (by deleting entire rows from my new dictionary)\n",
    "\n",
    "This means that I do not capture absolutely **all** of the dataset, but I get far more of it by pulling out errors than I would by manually trying to find every GPS coordinate for 3xx stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stations_turn = sorted(list(clean_df['station'].unique()))\n",
    "unique_stations_station = sorted(list(station_df['Stop Name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_stations_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_stations_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = {'Station': unique_stations_station, 'Turnstile': unique_stations_turn}\n",
    "df_combined = pd.DataFrame(data = df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to make a dictionary mapping station names to turnstile names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.set_index('Turnstile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turnstile</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1 AV</td>\n",
       "      <td>1 AV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103 ST</td>\n",
       "      <td>103 ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103 STCORONA</td>\n",
       "      <td>103 ST CORONA PLAZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104 ST</td>\n",
       "      <td>104 ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110 ST</td>\n",
       "      <td>110 ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>WOODLAWN</td>\n",
       "      <td>WOODLAWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>WORLD TRADE CTR</td>\n",
       "      <td>WORLD TRADE CENTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>WTCCORTLANDT</td>\n",
       "      <td>WTC CORTLANDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>YORK ST</td>\n",
       "      <td>YORK ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ZEREGA AV</td>\n",
       "      <td>ZEREGA AV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>347 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Station\n",
       "Turnstile                           \n",
       "1 AV                            1 AV\n",
       "103 ST                        103 ST\n",
       "103 STCORONA     103 ST CORONA PLAZA\n",
       "104 ST                        104 ST\n",
       "110 ST                        110 ST\n",
       "...                              ...\n",
       "WOODLAWN                    WOODLAWN\n",
       "WORLD TRADE CTR   WORLD TRADE CENTER\n",
       "WTCCORTLANDT           WTC CORTLANDT\n",
       "YORK ST                      YORK ST\n",
       "ZEREGA AV                  ZEREGA AV\n",
       "\n",
       "[347 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary of unequal items (items which are the same station but spelled differently or abbreviated differently). This will be used when I need to map these items from one dataset to the other in order to make them both equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unequal = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate(df_combined['Station']):\n",
    "     if df_combined.index[i] != v:\n",
    "            all_unequal[df_combined.index[i]] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_unequal[\"E 143 ST MARY'S\"]\n",
    "del all_unequal['E 149 ST']\n",
    "del all_unequal['E 180 ST']\n",
    "del all_unequal['EAST 105 ST']\n",
    "del all_unequal['DELANCEY ESSEX']\n",
    "del all_unequal['EXCHANGE PLACE']\n",
    "del all_unequal['EUCLID AV']\n",
    "del all_unequal['GRAND ST']\n",
    "del all_unequal['GRANT AV']\n",
    "del all_unequal['GRANDNEWTOWN']\n",
    "all_unequal['GRD CNTRL42 ST'] = 'GRAND CENTRAL 42 ST'\n",
    "del all_unequal['JEFFERSON ST']\n",
    "del all_unequal['JKSN HTROOSVLT']\n",
    "del all_unequal['LEXINGTON AV 53']\n",
    "del all_unequal['NEW LOTS']\n",
    "del all_unequal['RITROOSEVELT']\n",
    "del all_unequal['ST LAWRENCE AV']\n",
    "del all_unequal['ST. GEORGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'103 STCORONA': '103 ST CORONA PLAZA',\n",
       " '116 STCOLUMBIA': '116 ST COLUMBIA UNIVERSITY',\n",
       " '137 ST CITY COL': '137 ST CITY COLLEGE',\n",
       " '138 GRAND CONC': '138 ST GRAND CONCOURSE',\n",
       " '14 STUNION SQ': '14 ST UNION SQ',\n",
       " '149 GRAND CONC': '149 ST GRAND CONCOURSE',\n",
       " '15 STPROSPECT': '15 ST PROSPECT PARK',\n",
       " '161 YANKEE STAD': '161 ST YANKEE STADIUM',\n",
       " '163 STAMSTERDM': '163 ST AMSTERDAM AV',\n",
       " '21 STQNSBRIDGE': '21 ST QUEENSBRIDGE',\n",
       " '3 AV149 ST': '3 AV 149 ST',\n",
       " '33 STRAWSON ST': '34 ST 11 AV',\n",
       " '34 STHERALD SQ': '34 ST HERALD SQ',\n",
       " '34 STPENN STA': '34 ST PENN STATION',\n",
       " '4 AV9 ST': '4 AV',\n",
       " '40 ST LOWERY ST': '40 ST',\n",
       " '42 STBRYANT PK': '42 ST BRYANT PK',\n",
       " '42 STPORT AUTH': '42 ST PORT AUTHORITY BUS TERMINAL',\n",
       " '4750 STS ROCK': '4750 STS ROCKEFELLER CTR',\n",
       " '57 ST7 AV': '57 ST 7 AV',\n",
       " '59 ST COLUMBUS': '59 ST COLUMBUS CIRCLE',\n",
       " '61 ST WOODSIDE': '62 ST',\n",
       " '63 DRREGO PARK': '63 DR REGO PARK',\n",
       " '66 STLINCOLN': '66 ST LINCOLN CENTER',\n",
       " '68STHUNTER CO': '68 ST HUNTER COLLEGE',\n",
       " '74 STBROADWAY': '74 ST BROADWAY',\n",
       " '75 STELDERTS': '75 ST',\n",
       " '8 STNYU': '8 ST NYU',\n",
       " '81 STMUSEUM': '81 ST MUSEUM OF NATURAL HISTORY',\n",
       " '82 STJACKSON H': '82 ST JACKSON HTS',\n",
       " '85 STFOREST PK': '85 ST FOREST PKWY',\n",
       " '90 STELMHURST': '90 ST ELMHURST AV',\n",
       " 'AQUEDUCT N.COND': 'AQUEDUCT N CONDUIT AV',\n",
       " 'AQUEDUCT RACETR': 'AQUEDUCT RACETRACK',\n",
       " 'ASTORIA DITMARS': 'ASTORIA DITMARS BLVD',\n",
       " 'ATL AVBARCLAY': 'ATLANTIC AV',\n",
       " 'ATLANTIC AV': 'ATLANTIC AV BARCLAYS CTR',\n",
       " 'BEVERLEY ROAD': 'BEVERLEY RD',\n",
       " 'BRIARWOOD': 'BRIARWOOD VAN WYCK BLVD',\n",
       " 'BROOKLYN BRIDGE': 'BROOKLYN BRIDGE CITY HALL',\n",
       " 'BUSHWICK AV': 'BUSHWICK AV ABERDEEN ST',\n",
       " 'CANARSIEROCKAW': 'CANARSIE ROCKAWAY PKWY',\n",
       " 'CENTRAL PK N110': 'CENTRAL PARK NORTH (110 ST)',\n",
       " 'CHRISTOPHER ST': 'CHRISTOPHER ST SHERIDAN SQ',\n",
       " 'CLINTONWASH AV': 'CLINTON WASHINGTON AV',\n",
       " 'CONEY ISSTILLW': 'CONEY ISLAND STILLWELL AV',\n",
       " 'COURT SQ23 ST': 'COURT ST',\n",
       " 'CROWN HTSUTICA': 'CROWN HTS UTICA AV',\n",
       " 'EASTCHSTER DYRE': 'EASTCHESTER DYRE AV',\n",
       " 'EASTN PKWYMUSM': 'EASTERN PKWY BROOKLYN MUSEUM',\n",
       " 'FAR ROCKAWAY': 'FAR ROCKAWAY MOTT AV',\n",
       " 'FLATBUSH AVB.C': 'FLATBUSH AV BROOKLYN COLLEGE',\n",
       " 'FLUSHINGMAIN': 'FLUSHING MAIN ST',\n",
       " 'FOREST HILLS 71': 'FOREST HILLS 71 AV',\n",
       " 'FRANKLIN AV': 'FORT HAMILTON PKWY',\n",
       " 'FRANKLIN ST': 'FRANKLIN AV',\n",
       " 'FREEMAN ST': 'FRANKLIN ST',\n",
       " 'FRESH POND RD': 'FREEMAN ST',\n",
       " 'FT HAMILTON PKY': 'FRESH POND RD',\n",
       " 'GRAND ARMY PLAZ': 'GRAND ARMY PLAZA',\n",
       " 'GRD CNTRL42 ST': 'GRAND CENTRAL 42 ST',\n",
       " 'HOWARD BCH JFK': 'HOWARD BEACH JFK AIRPORT',\n",
       " 'HOYTSCHER': 'HUGUENOT',\n",
       " 'HUNTERS PT AV': 'HUNTERS POINT AV',\n",
       " 'INWOOD207 ST': 'INWOOD 207 ST',\n",
       " 'JAMAICA CENTER': 'JAMAICA CENTER PARSONS ARCHER',\n",
       " 'JAMAICA VAN WK': 'JAMAICA VAN WYCK',\n",
       " 'JAY STMETROTEC': 'JAY ST METROTECH',\n",
       " 'KEW GARDENS': 'KEW GARDENS UNION TPKE',\n",
       " 'KINGSTONTHROOP': 'KINGSTON THROOP AV',\n",
       " 'KNICKERBOCKER': 'KNICKERBOCKER AV',\n",
       " 'LEXINGTON AV 63': 'LEXINGTON AV 63 ST',\n",
       " 'MARBLE HILL225': 'MARBLE HILL 225 ST',\n",
       " 'METSWILLETS PT': 'METS WILLETS POINT',\n",
       " 'MORISN AV SNDVW': 'MORRIS PARK',\n",
       " 'MORRIS PARK': 'MORRISON AV SOUND VIEW',\n",
       " 'MYRTLEWILLOUGH': 'MYRTLE WILLOUGHBY AV',\n",
       " 'MYRTLEWYCKOFF': 'MYRTLE WYCKOFF AV',\n",
       " 'ORCHARD BEACH': 'OLD TOWN',\n",
       " 'OZONE PK LEFFRT': 'OZONE PARK LEFFERTS BLVD',\n",
       " 'PARK PLACE': 'PARK PL',\n",
       " 'QUEENSBORO PLZ': 'QUEENSBORO PLAZA',\n",
       " 'ROCKAWAY PARK B': 'ROCKAWAY PARK BEACH 116 ST',\n",
       " 'ROOSEVELT ISLND': 'ROOSEVELT ISLAND',\n",
       " 'SMITH9 ST': 'SMITH 9 STS',\n",
       " 'SUTTER AVRUTLD': 'SUTTER AV RUTLAND RD',\n",
       " 'TIMES SQ42 ST': 'TIMES SQ 42 ST',\n",
       " 'V.CORTLANDT PK': 'VAN CORTLANDT PARK 242 ST',\n",
       " 'VERNONJACKSON': 'VERNON BLVD JACKSON AV',\n",
       " 'W 4 STWASH SQ': 'W 4 ST',\n",
       " 'W 8 STAQUARIUM': 'W 8 ST NY AQUARIUM',\n",
       " 'WAKEFIELD 241': 'WAKEFIELD 241 ST',\n",
       " 'WEST FARMS SQ': 'WEST FARMS SQ E TREMONT AV',\n",
       " 'WESTCHESTER SQ': 'WESTCHESTER SQ E TREMONT AV',\n",
       " 'WHITEHALL SFRY': 'WHITEHALL ST',\n",
       " 'WORLD TRADE CTR': 'WORLD TRADE CENTER',\n",
       " 'WTCCORTLANDT': 'WTC CORTLANDT'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unequal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Columns:\n",
    "#### Finally....a dictionary to map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_df['station'] = clean_df['station'].replace(all_unequal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating copies, because merging is hard and I want to be able to go back to my clean dfs\n",
    "station_df_copy = station_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_copy = clean_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to do a .join() the column names need to be equal\n",
    "station_df_copy = station_df_copy.rename(columns = {'Stop Name':'station'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I discovered that there were duplicate stations in the stations_df causing the merge to add a million rows\n",
    "station_df_copy = station_df_copy.drop_duplicates('station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to remove more problems from the merge by dropping this useless column\n",
    "station_df_copy = station_df_copy.drop('Station ID', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting:\n",
    "\n",
    "#### Figuring out that there were duplicates in the stop name column, dropping nas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.reset_index().dropna()\n",
    "\n",
    "# missing = []\n",
    "# for column in combined.columns:\n",
    "#     bad_indices = combined[combined[column].isna()].index.values\n",
    "#     missing.extend(bad_indices)\n",
    "# missing = set(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2067574, 15)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119.]\n",
      "[156. 309. 395.]\n",
      "[450.]\n",
      "[ 82. 193.]\n",
      "[394.]\n",
      "[ 81. 194. 449.]\n",
      "[154. 393. 440.]\n",
      "[307.]\n",
      "[80.]\n",
      "[153. 306. 392. 439.]\n",
      "[152. 438.]\n",
      "[305.]\n",
      "[391.]\n",
      "[166. 229. 322.]\n",
      "[ 15. 406.]\n",
      "[151. 304. 437.]\n",
      "[390. 435.]\n",
      "[241.]\n",
      "[150. 220.]\n",
      "[303.]\n",
      "[219. 389.]\n",
      "[149.]\n",
      "[218. 388.]\n",
      "[148.]\n",
      "[255.]\n",
      "[217. 387.]\n",
      "[428.]\n",
      "[216.]\n",
      "[147.]\n",
      "[385.]\n",
      "[ 66.  74. 245.]\n",
      "[321.]\n",
      "[146. 301.]\n",
      "[214.]\n",
      "[383.]\n",
      "[145.]\n",
      "[300.]\n",
      "[232.]\n",
      "[67. 75.]\n",
      "[298.]\n",
      "[282.]\n",
      "[221.]\n",
      "[297.]\n",
      "[420.]\n",
      "[419.]\n",
      "[ 14. 165. 228. 320. 405.]\n",
      "[295.]\n",
      "[418.]\n",
      "[294.]\n",
      "[69.]\n",
      "[31.]\n",
      "[ 13. 319. 404.]\n",
      "[118.]\n",
      "[377.]\n",
      "[434.]\n",
      "[3.]\n",
      "[403. 460.]\n",
      "[471.]\n",
      "[ 12. 227.]\n",
      "[164. 318.]\n",
      "[5.]\n",
      "[ 32. 272.]\n",
      "[6.]\n",
      "[239.]\n",
      "[459.]\n",
      "[226.]\n",
      "[163.]\n",
      "[33.]\n",
      "[270. 458.]\n",
      "[225.]\n",
      "[10.]\n",
      "[466.]\n",
      "[276.]\n",
      "[8.]\n",
      "[ 61. 162. 316.]\n",
      "[401.]\n",
      "[457.]\n",
      "[34.]\n",
      "[62.]\n",
      "[224.]\n",
      "[9.]\n",
      "[ 35. 400.]\n",
      "[161. 315.]\n",
      "[116.]\n",
      "[63.]\n",
      "[263.]\n",
      "[268.]\n",
      "[314.]\n",
      "[262.]\n",
      "[399.]\n",
      "[455.]\n",
      "[ 41. 240. 277.]\n",
      "[64.]\n",
      "[160. 313. 477.]\n",
      "[454.]\n",
      "[260.]\n",
      "[85.]\n",
      "[ 37. 398.]\n",
      "[ 65. 312.]\n",
      "[ 71. 115.]\n",
      "[16.]\n",
      "[190.]\n",
      "[159.]\n",
      "[453.]\n",
      "[84.]\n",
      "[ 38.  79. 158. 311. 397. 476.]\n",
      "[191.]\n",
      "[59.]\n",
      "[452.]\n",
      "[157. 310. 396. 475.]\n",
      "[91.]\n",
      "[423.]\n",
      "[197.]\n",
      "[196.]\n",
      "[407.]\n",
      "[2.]\n",
      "[1.]\n",
      "[133.]\n",
      "[ 27.  40. 338.]\n",
      "[48.]\n",
      "[246.]\n",
      "[49.]\n",
      "[50.]\n",
      "[248.]\n",
      "[249.]\n",
      "[ 52.  78. 251.]\n",
      "[252.]\n",
      "[70.]\n",
      "[ 68.  76. 247.]\n",
      "[443.]\n",
      "[202.]\n",
      "[208.]\n",
      "[207.]\n",
      "[206.]\n",
      "[205.]\n",
      "[204.]\n",
      "[200.]\n",
      "[201.]\n",
      "[120.]\n",
      "[236. 339.]\n",
      "[45.]\n",
      "[357.]\n",
      "[408.]\n",
      "[335. 415.]\n",
      "[142.]\n",
      "[103.]\n",
      "[414.]\n",
      "[258.]\n",
      "[55.]\n",
      "[199.]\n",
      "[107.]\n",
      "[  4. 286.]\n",
      "[ 92. 132. 184.]\n",
      "[425.]\n",
      "[376.]\n",
      "[411.]\n",
      "[361.]\n",
      "[422.]\n",
      "[384.]\n",
      "[131.]\n",
      "[ 18.  19. 104. 169. 325. 410.]\n",
      "[138.]\n",
      "[237.]\n",
      "[365.]\n",
      "[308.]\n",
      "[114.]\n",
      "[441.]\n",
      "[105. 170. 327.]\n",
      "[93.]\n",
      "[323.]\n",
      "[ 44. 243. 356.]\n",
      "[20.]\n",
      "[334.]\n",
      "[290.]\n",
      "[89.]\n",
      "[177. 291.]\n",
      "[58.]\n",
      "[46.]\n",
      "[21.]\n",
      "[274. 281. 462.]\n",
      "[24.]\n",
      "[87.]\n",
      "[345.]\n",
      "[375.]\n",
      "[86.]\n",
      "[ 26. 127.]\n",
      "[244.]\n",
      "[144. 299.]\n",
      "[373.]\n",
      "[426.]\n",
      "[234.]\n",
      "[442.]\n",
      "[341.]\n",
      "[369.]\n",
      "[266.]\n",
      "[188.]\n",
      "[209.]\n",
      "[359.]\n",
      "[ 98. 287.]\n",
      "[447.]\n",
      "[213. 382.]\n",
      "[110.]\n",
      "[261.]\n",
      "[ 60.  72. 242.]\n",
      "[139. 178. 342.]\n",
      "[326.]\n",
      "[429.]\n",
      "[109.]\n",
      "[106. 172. 292. 332. 412.]\n",
      "[95.]\n",
      "[122.]\n",
      "[340.]\n",
      "[402. 465. 469.]\n",
      "[123. 231.]\n",
      "[189.]\n",
      "[283.]\n",
      "[421. 444.]\n",
      "[ 94. 129.]\n",
      "[436.]\n",
      "[100.]\n",
      "[173.]\n",
      "[324.]\n",
      "[198.]\n",
      "[336.]\n",
      "[516.]\n",
      "[463.]\n",
      "[371.]\n",
      "[431.]\n",
      "[143.]\n",
      "[433.]\n",
      "[254.]\n",
      "[278.]\n",
      "[280.]\n",
      "[ 25. 174.]\n",
      "[126.]\n",
      "[451.]\n",
      "[349.]\n",
      "[259.]\n",
      "[ 51.  77. 250.]\n",
      "[212. 381.]\n",
      "[344.]\n",
      "[180.]\n",
      "[113.]\n",
      "[96.]\n",
      "[176.]\n",
      "[223.]\n",
      "[185.]\n",
      "[135.]\n",
      "[372.]\n",
      "[ 99. 121.]\n",
      "[296.]\n",
      "[101.]\n",
      "[285.]\n",
      "[448.]\n",
      "[362.]\n",
      "[124.]\n",
      "[125.]\n",
      "[446.]\n",
      "[368.]\n",
      "[379.]\n",
      "[386.]\n",
      "[97.]\n",
      "[288.]\n",
      "[112. 128.]\n",
      "[284.]\n",
      "[53.]\n",
      "[253.]\n",
      "[417.]\n",
      "[337.]\n",
      "[136. 352.]\n",
      "[73.]\n",
      "[358.]\n",
      "[47.]\n",
      "[269.]\n",
      "[210.]\n",
      "[88.]\n",
      "[179. 343.]\n",
      "[56.]\n",
      "[506.]\n",
      "[195.]\n",
      "[141. 331.]\n",
      "[366.]\n",
      "[43.]\n",
      "[256.]\n",
      "[360.]\n",
      "[424. 445.]\n",
      "[350.]\n",
      "[353.]\n",
      "[17.]\n",
      "[ 30. 432.]\n",
      "[42.]\n",
      "[273.]\n",
      "[461.]\n",
      "[182.]\n",
      "[ 22. 329.]\n",
      "[183. 348.]\n",
      "[192.]\n",
      "[203.]\n",
      "[222.]\n",
      "[347.]\n",
      "[111.]\n",
      "[54.]\n",
      "[187.]\n",
      "[430.]\n",
      "[238.]\n",
      "[330.]\n",
      "[168. 409.]\n",
      "[367.]\n",
      "[271.]\n",
      "[354.]\n",
      "[257.]\n",
      "[134.]\n",
      "[346.]\n",
      "[ 11. 317. 467. 468.]\n",
      "[502.]\n",
      "[215.]\n",
      "[28.]\n",
      "[181.]\n",
      "[293.]\n",
      "[ 90. 186. 351.]\n",
      "[464.]\n",
      "[167.]\n",
      "[57.]\n",
      "[416.]\n",
      "[333. 413.]\n",
      "[427.]\n",
      "[363.]\n",
      "[23.]\n",
      "[370.]\n",
      "[130.]\n",
      "[355.]\n",
      "[ 83. 264.]\n",
      "[378.]\n",
      "[171.]\n",
      "[328.]\n",
      "[235.]\n",
      "[364.]\n"
     ]
    }
   ],
   "source": [
    "# stations = combined['station'].unique()\n",
    " \n",
    "# for station in stations:\n",
    " #   print(combined[combined['station'] == station]['Station ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>entries</th>\n",
       "      <th>exits</th>\n",
       "      <th>date_time</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>turnstile_id</th>\n",
       "      <th>exit_counts</th>\n",
       "      <th>entry_counts</th>\n",
       "      <th>total_traffic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>6470223</td>\n",
       "      <td>2190140</td>\n",
       "      <td>2018-01-06 07:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>6470256</td>\n",
       "      <td>2190229</td>\n",
       "      <td>2018-01-06 11:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>6470379</td>\n",
       "      <td>2190299</td>\n",
       "      <td>2018-01-06 15:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>6470665</td>\n",
       "      <td>2190366</td>\n",
       "      <td>2018-01-06 19:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>6470809</td>\n",
       "      <td>2190398</td>\n",
       "      <td>2018-01-06 23:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2284152</td>\n",
       "      <td>RITROOSEVELT</td>\n",
       "      <td>5554</td>\n",
       "      <td>379</td>\n",
       "      <td>2019-06-05 17:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>2019</td>\n",
       "      <td>9686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2284153</td>\n",
       "      <td>RITROOSEVELT</td>\n",
       "      <td>5554</td>\n",
       "      <td>379</td>\n",
       "      <td>2019-06-05 21:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>2019</td>\n",
       "      <td>9686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2284154</td>\n",
       "      <td>RITROOSEVELT</td>\n",
       "      <td>5554</td>\n",
       "      <td>379</td>\n",
       "      <td>2019-06-06 01:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>9686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2284155</td>\n",
       "      <td>RITROOSEVELT</td>\n",
       "      <td>5554</td>\n",
       "      <td>379</td>\n",
       "      <td>2019-06-06 05:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>9686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2284156</td>\n",
       "      <td>RITROOSEVELT</td>\n",
       "      <td>5554</td>\n",
       "      <td>379</td>\n",
       "      <td>2019-06-06 09:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2019</td>\n",
       "      <td>9686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2067574 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              station  entries    exits           date_time  weekday  hour  \\\n",
       "0               59 ST  6470223  2190140 2018-01-06 07:00:00        5     7   \n",
       "1               59 ST  6470256  2190229 2018-01-06 11:00:00        5    11   \n",
       "2               59 ST  6470379  2190299 2018-01-06 15:00:00        5    15   \n",
       "3               59 ST  6470665  2190366 2018-01-06 19:00:00        5    19   \n",
       "4               59 ST  6470809  2190398 2018-01-06 23:00:00        5    23   \n",
       "...               ...      ...      ...                 ...      ...   ...   \n",
       "2284152  RITROOSEVELT     5554      379 2019-06-05 17:00:00        2    17   \n",
       "2284153  RITROOSEVELT     5554      379 2019-06-05 21:00:00        2    21   \n",
       "2284154  RITROOSEVELT     5554      379 2019-06-06 01:00:00        3     1   \n",
       "2284155  RITROOSEVELT     5554      379 2019-06-06 05:00:00        3     5   \n",
       "2284156  RITROOSEVELT     5554      379 2019-06-06 09:00:00        3     9   \n",
       "\n",
       "         year  turnstile_id  exit_counts  entry_counts  total_traffic  \n",
       "0        2018             0         17.0           7.0           24.0  \n",
       "1        2018             0         89.0          33.0          122.0  \n",
       "2        2018             0         70.0         123.0          193.0  \n",
       "3        2018             0         67.0         286.0          353.0  \n",
       "4        2018             0         32.0         144.0          176.0  \n",
       "...       ...           ...          ...           ...            ...  \n",
       "2284152  2019          9686          0.0           0.0            0.0  \n",
       "2284153  2019          9686          0.0           0.0            0.0  \n",
       "2284154  2019          9686          0.0           0.0            0.0  \n",
       "2284155  2019          9686          0.0           0.0            0.0  \n",
       "2284156  2019          9686          0.0           0.0            0.0  \n",
       "\n",
       "[2067574 rows x 11 columns]"
      ]
     },
     "execution_count": 1007,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean_df_copy.join(station_df_copy, on = 'station', how = 'right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging:\n",
    "#### It Works! A combined df of stations, turnstiles, gps coordinates, and tech hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined = clean_df_copy.set_index('station').join(station_df_copy.set_index('station'), how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entries</th>\n",
       "      <th>exits</th>\n",
       "      <th>date_time</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>turnstile_id</th>\n",
       "      <th>exit_counts</th>\n",
       "      <th>entry_counts</th>\n",
       "      <th>total_traffic</th>\n",
       "      <th>Division</th>\n",
       "      <th>Borough</th>\n",
       "      <th>GTFS Latitude</th>\n",
       "      <th>GTFS Longitude</th>\n",
       "      <th>tech_borough</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1 AV</td>\n",
       "      <td>13080575</td>\n",
       "      <td>14585697</td>\n",
       "      <td>2018-01-06 07:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2018</td>\n",
       "      <td>1491</td>\n",
       "      <td>120.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>BMT</td>\n",
       "      <td>M</td>\n",
       "      <td>40.730953</td>\n",
       "      <td>-73.981628</td>\n",
       "      <td>flatiron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1 AV</td>\n",
       "      <td>13080870</td>\n",
       "      <td>14586195</td>\n",
       "      <td>2018-01-06 11:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>1491</td>\n",
       "      <td>498.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>BMT</td>\n",
       "      <td>M</td>\n",
       "      <td>40.730953</td>\n",
       "      <td>-73.981628</td>\n",
       "      <td>flatiron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1 AV</td>\n",
       "      <td>13081558</td>\n",
       "      <td>14587007</td>\n",
       "      <td>2018-01-06 15:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2018</td>\n",
       "      <td>1491</td>\n",
       "      <td>812.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>BMT</td>\n",
       "      <td>M</td>\n",
       "      <td>40.730953</td>\n",
       "      <td>-73.981628</td>\n",
       "      <td>flatiron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1 AV</td>\n",
       "      <td>13082789</td>\n",
       "      <td>14588521</td>\n",
       "      <td>2018-01-06 23:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>2018</td>\n",
       "      <td>1491</td>\n",
       "      <td>591.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>BMT</td>\n",
       "      <td>M</td>\n",
       "      <td>40.730953</td>\n",
       "      <td>-73.981628</td>\n",
       "      <td>flatiron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1 AV</td>\n",
       "      <td>13083004</td>\n",
       "      <td>14588781</td>\n",
       "      <td>2018-01-07 03:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "      <td>1491</td>\n",
       "      <td>260.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>BMT</td>\n",
       "      <td>M</td>\n",
       "      <td>40.730953</td>\n",
       "      <td>-73.981628</td>\n",
       "      <td>flatiron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ZEREGA AV</td>\n",
       "      <td>39</td>\n",
       "      <td>148</td>\n",
       "      <td>2019-06-07 05:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>8530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IRT</td>\n",
       "      <td>Bx</td>\n",
       "      <td>40.836488</td>\n",
       "      <td>-73.847036</td>\n",
       "      <td>outside_tech_hub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ZEREGA AV</td>\n",
       "      <td>39</td>\n",
       "      <td>148</td>\n",
       "      <td>2019-06-07 09:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2019</td>\n",
       "      <td>8530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IRT</td>\n",
       "      <td>Bx</td>\n",
       "      <td>40.836488</td>\n",
       "      <td>-73.847036</td>\n",
       "      <td>outside_tech_hub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ZEREGA AV</td>\n",
       "      <td>39</td>\n",
       "      <td>148</td>\n",
       "      <td>2019-06-07 13:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>2019</td>\n",
       "      <td>8530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IRT</td>\n",
       "      <td>Bx</td>\n",
       "      <td>40.836488</td>\n",
       "      <td>-73.847036</td>\n",
       "      <td>outside_tech_hub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ZEREGA AV</td>\n",
       "      <td>39</td>\n",
       "      <td>148</td>\n",
       "      <td>2019-06-07 17:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>2019</td>\n",
       "      <td>8530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IRT</td>\n",
       "      <td>Bx</td>\n",
       "      <td>40.836488</td>\n",
       "      <td>-73.847036</td>\n",
       "      <td>outside_tech_hub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ZEREGA AV</td>\n",
       "      <td>39</td>\n",
       "      <td>148</td>\n",
       "      <td>2019-06-07 21:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>2019</td>\n",
       "      <td>8530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IRT</td>\n",
       "      <td>Bx</td>\n",
       "      <td>40.836488</td>\n",
       "      <td>-73.847036</td>\n",
       "      <td>outside_tech_hub</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2067574 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            entries     exits           date_time  weekday  hour  year  \\\n",
       "station                                                                  \n",
       "1 AV       13080575  14585697 2018-01-06 07:00:00        5     7  2018   \n",
       "1 AV       13080870  14586195 2018-01-06 11:00:00        5    11  2018   \n",
       "1 AV       13081558  14587007 2018-01-06 15:00:00        5    15  2018   \n",
       "1 AV       13082789  14588521 2018-01-06 23:00:00        5    23  2018   \n",
       "1 AV       13083004  14588781 2018-01-07 03:00:00        6     3  2018   \n",
       "...             ...       ...                 ...      ...   ...   ...   \n",
       "ZEREGA AV        39       148 2019-06-07 05:00:00        4     5  2019   \n",
       "ZEREGA AV        39       148 2019-06-07 09:00:00        4     9  2019   \n",
       "ZEREGA AV        39       148 2019-06-07 13:00:00        4    13  2019   \n",
       "ZEREGA AV        39       148 2019-06-07 17:00:00        4    17  2019   \n",
       "ZEREGA AV        39       148 2019-06-07 21:00:00        4    21  2019   \n",
       "\n",
       "           turnstile_id  exit_counts  entry_counts  total_traffic Division  \\\n",
       "station                                                                      \n",
       "1 AV               1491        120.0          31.0          151.0      BMT   \n",
       "1 AV               1491        498.0         295.0          793.0      BMT   \n",
       "1 AV               1491        812.0         688.0         1500.0      BMT   \n",
       "1 AV               1491        591.0         501.0         1092.0      BMT   \n",
       "1 AV               1491        260.0         215.0          475.0      BMT   \n",
       "...                 ...          ...           ...            ...      ...   \n",
       "ZEREGA AV          8530          0.0           0.0            0.0      IRT   \n",
       "ZEREGA AV          8530          0.0           0.0            0.0      IRT   \n",
       "ZEREGA AV          8530          0.0           0.0            0.0      IRT   \n",
       "ZEREGA AV          8530          0.0           0.0            0.0      IRT   \n",
       "ZEREGA AV          8530          0.0           0.0            0.0      IRT   \n",
       "\n",
       "          Borough  GTFS Latitude  GTFS Longitude      tech_borough  \n",
       "station                                                             \n",
       "1 AV            M      40.730953      -73.981628          flatiron  \n",
       "1 AV            M      40.730953      -73.981628          flatiron  \n",
       "1 AV            M      40.730953      -73.981628          flatiron  \n",
       "1 AV            M      40.730953      -73.981628          flatiron  \n",
       "1 AV            M      40.730953      -73.981628          flatiron  \n",
       "...           ...            ...             ...               ...  \n",
       "ZEREGA AV      Bx      40.836488      -73.847036  outside_tech_hub  \n",
       "ZEREGA AV      Bx      40.836488      -73.847036  outside_tech_hub  \n",
       "ZEREGA AV      Bx      40.836488      -73.847036  outside_tech_hub  \n",
       "ZEREGA AV      Bx      40.836488      -73.847036  outside_tech_hub  \n",
       "ZEREGA AV      Bx      40.836488      -73.847036  outside_tech_hub  \n",
       "\n",
       "[2067574 rows x 15 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling the final combined df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/combined_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(combined, to_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
