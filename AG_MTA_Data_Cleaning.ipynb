{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Reading in the Turnstile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['180113',\n",
       " '180210',\n",
       " '180310',\n",
       " '180414',\n",
       " '180512',\n",
       " '180609',\n",
       " '190112',\n",
       " '190209',\n",
       " '190309',\n",
       " '190413',\n",
       " '190511',\n",
       " '190608']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the weeks to retrieve from the MTA database\n",
    "def list_weeks(list_of_dates):\n",
    "    weeks = []\n",
    "    for date in list_of_dates:\n",
    "        week_string = date.strftime('%y%m%d')\n",
    "        weeks.append(week_string)\n",
    "    return weeks        \n",
    "\n",
    "# We want to pull in data from a week each month January - June in 2018 and 2019\n",
    "# dates_\n",
    "# dates = list(map(dt.datetime,))\n",
    "dates = [dt.datetime(2018,1,13),dt.datetime(2018,2,10),dt.datetime(2018,3,10),dt.datetime(2018,4,14),\\\n",
    "         dt.datetime(2018,5,12),dt.datetime(2018,6,9),\\\n",
    "         dt.datetime(2019,1,12),dt.datetime(2019,2,9),dt.datetime(2019,3,9),dt.datetime(2019,4,13),\\\n",
    "        dt.datetime(2019,5,11),dt.datetime(2019,6,8)]\n",
    "weeks = list_weeks(dates)\n",
    "weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data for the desired weeks\n",
    "def readTurnstileData(week_string):\n",
    "    \"\"\"\n",
    "    This function reads in data from an online MTA Turnstile dataset into a DataFrame\n",
    "    ---\n",
    "    input: link to dataset\n",
    "    output: DataFrame\n",
    "    \"\"\"\n",
    "    cols = ['control_area','unit','scp','station','line_name','division','date','time',\n",
    "        'desc','entries','exits']\n",
    "    \n",
    "    link = 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_' + week_string + '.txt'\n",
    "    turnstile_data = pd.read_csv(link, header = 0, names = cols)\n",
    "    return turnstile_data\n",
    "\n",
    "# Create a single DataFrame containing all weeks\n",
    "def createTurnstileDataFrame(list_of_weeks):\n",
    "    df = pd.DataFrame()\n",
    "    df_chunk_list = []\n",
    "    for date in list_of_weeks:\n",
    "        df_chunk = readTurnstileData(date)\n",
    "        df_chunk_list.append(df_chunk)\n",
    "    df = pd.concat(df_chunk_list)\n",
    "    return df\n",
    "\n",
    "# Clean up date/time info\n",
    "def formatDateTime(df):\n",
    "    \"\"\"\n",
    "    This function converts the date and time into DateTime format in a single column\n",
    "    and deletes the unformatted date and time columns\n",
    "    \n",
    "    Note: only run once per DataFrame, will result in error otherwise\n",
    "    ---\n",
    "    input: DataFrame\n",
    "    output: DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    #convert date and time to DateTime format in a single column\n",
    "    df['concat_date_time'] = df['date'] + ' ' + df['time']\n",
    "    df['date_time'] = pd.to_datetime(df.concat_date_time,format = '%m/%d/%Y %H:%M:%S')\n",
    "    \n",
    "    #delete unformatted date and time columns\n",
    "    del df['concat_date_time']\n",
    "    del df['date']\n",
    "    del df['time']\n",
    "    return df\n",
    "\n",
    "def read_and_format_turnstile_data(list_of_weeks):\n",
    "    \"\"\"\n",
    "    This function reads in Turnstile data from online and returns a DataFrame with \n",
    "    with the date and time information converted to a single DateTime column\n",
    "    ---\n",
    "    input: link\n",
    "    output: DataFrame\n",
    "    \"\"\"\n",
    "    df = createTurnstileDataFrame(list_of_weeks)\n",
    "    df = formatDateTime(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the specified weeks\n",
    "orig_df = read_and_format_turnstile_data(weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for weekday\n",
    "orig_df['weekday'] = orig_df[['date_time']].apply(lambda x: x['date_time'].dayofweek,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for the time of day\n",
    "orig_df['hour'] = orig_df[['date_time']].apply(lambda x: x['date_time'].hour,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for year\n",
    "orig_df['year'] = orig_df[['date_time']].apply(lambda x: x['date_time'].year,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with unique ID for each turnstile\n",
    "orig_df['turnstile_id'] = orig_df.groupby(['control_area','unit','scp','station','year']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indexing on the DataFrame\n",
    "orig_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns that won't be used\n",
    "orig_df.drop(columns=['index','control_area','unit','scp','line_name','division','desc'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw data to access easily later\n",
    "with open('data/orig_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(orig_df, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2ish: Read in Station Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: read in data from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the pickle file if you are starting here\n",
    "# If you started from the beginning of the notebook, comment out the code below\n",
    "\n",
    "with open('data/orig_df.pickle','rb') as read_file:\n",
    "    orig_df = pickle.load(read_file)\n",
    "df = orig_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Organizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2409320, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for latitude/longitude and borough of each turnstile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by turnstile and date\n",
    "df.sort_values(['turnstile_id','date_time'],inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Find entries and exits differences per turnstile\n",
    "df['exit_counts'] = abs(df.groupby('turnstile_id').exits.diff())\n",
    "df['entry_counts'] = abs(df.groupby('turnstile_id').entries.diff())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entries</th>\n",
       "      <th>exits</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>turnstile_id</th>\n",
       "      <th>exit_counts</th>\n",
       "      <th>entry_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.409320e+06</td>\n",
       "      <td>2.409320e+06</td>\n",
       "      <td>2.409320e+06</td>\n",
       "      <td>2.409320e+06</td>\n",
       "      <td>2.409320e+06</td>\n",
       "      <td>2.409320e+06</td>\n",
       "      <td>2.399633e+06</td>\n",
       "      <td>2.399633e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.021208e+07</td>\n",
       "      <td>3.306011e+07</td>\n",
       "      <td>2.990220e+00</td>\n",
       "      <td>1.112374e+01</td>\n",
       "      <td>2.018507e+03</td>\n",
       "      <td>4.856233e+03</td>\n",
       "      <td>2.506554e+04</td>\n",
       "      <td>3.522321e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.080115e+08</td>\n",
       "      <td>1.924366e+08</td>\n",
       "      <td>1.997990e+00</td>\n",
       "      <td>6.925758e+00</td>\n",
       "      <td>4.999505e-01</td>\n",
       "      <td>2.802334e+03</td>\n",
       "      <td>5.185810e+06</td>\n",
       "      <td>6.675077e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.018000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.587520e+05</td>\n",
       "      <td>2.037400e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.018000e+03</td>\n",
       "      <td>2.445000e+03</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.426018e+06</td>\n",
       "      <td>1.377258e+06</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>2.019000e+03</td>\n",
       "      <td>4.840000e+03</td>\n",
       "      <td>5.300000e+01</td>\n",
       "      <td>7.700000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.862688e+06</td>\n",
       "      <td>4.774284e+06</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>2.019000e+03</td>\n",
       "      <td>7.292000e+03</td>\n",
       "      <td>1.740000e+02</td>\n",
       "      <td>2.550000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.130144e+09</td>\n",
       "      <td>2.145850e+09</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>2.019000e+03</td>\n",
       "      <td>9.686000e+03</td>\n",
       "      <td>2.145033e+09</td>\n",
       "      <td>2.088495e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            entries         exits       weekday          hour          year  \\\n",
       "count  2.409320e+06  2.409320e+06  2.409320e+06  2.409320e+06  2.409320e+06   \n",
       "mean   4.021208e+07  3.306011e+07  2.990220e+00  1.112374e+01  2.018507e+03   \n",
       "std    2.080115e+08  1.924366e+08  1.997990e+00  6.925758e+00  4.999505e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  2.018000e+03   \n",
       "25%    4.587520e+05  2.037400e+05  1.000000e+00  5.000000e+00  2.018000e+03   \n",
       "50%    2.426018e+06  1.377258e+06  3.000000e+00  1.100000e+01  2.019000e+03   \n",
       "75%    6.862688e+06  4.774284e+06  5.000000e+00  1.700000e+01  2.019000e+03   \n",
       "max    2.130144e+09  2.145850e+09  6.000000e+00  2.300000e+01  2.019000e+03   \n",
       "\n",
       "       turnstile_id   exit_counts  entry_counts  \n",
       "count  2.409320e+06  2.399633e+06  2.399633e+06  \n",
       "mean   4.856233e+03  2.506554e+04  3.522321e+04  \n",
       "std    2.802334e+03  5.185810e+06  6.675077e+06  \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "25%    2.445000e+03  9.000000e+00  1.100000e+01  \n",
       "50%    4.840000e+03  5.300000e+01  7.700000e+01  \n",
       "75%    7.292000e+03  1.740000e+02  2.550000e+02  \n",
       "max    9.686000e+03  2.145033e+09  2.088495e+09  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max values on exit and entry counts are way too high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOutliers(df,column):\n",
    "    \"\"\"\n",
    "    Returns outliers above the max limit for a column in a dataframe\n",
    "    Adjust outlier cutoff to q75 + 4*iqr to include more data\n",
    "    ---\n",
    "    input: DataFrame, column\n",
    "    output: DataFrame\n",
    "    \"\"\"\n",
    "    q25,q50,q75 = df[column].quantile(q=[0.25,0.5,0.75])\n",
    "    iqr = q75-q25\n",
    "    #max limits to be considered an outlier\n",
    "    max = q75 + 4*iqr\n",
    "    #identify the points\n",
    "    outlier_mask = [True if x > max else False for x in df[column]]\n",
    "    print('{} outliers found out of {} data points, {}% of the data'.format(sum(outlier_mask),len(df[column]),100*(sum(outlier_mask)/len(df[column]))))\n",
    "    return outlier_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry: \n",
      "64486 outliers found out of 2409320 data points, 2.676522836318961% of the data\n",
      "\n",
      "Exit:\n",
      "92339 outliers found out of 2409320 data points, 3.832575166437003% of the data\n",
      "\n",
      "2344834 points on entry side left after removing entry_counts outlier points\n",
      "2316981 points on exit side left left after removing exit_counts outlier points\n"
     ]
    }
   ],
   "source": [
    "#Get outliers for entries\n",
    "print('Entry: ')\n",
    "df['entry_outliers'] = findOutliers(df,'entry_counts')\n",
    "\n",
    "#Get outliers for exits\n",
    "print('\\nExit:')\n",
    "df['exit_outliers'] = findOutliers(df,'exit_counts')\n",
    "\n",
    "#DataFrame with entry outliers removed\n",
    "clean_df_entries = df.loc[~df['entry_outliers']]\n",
    "print('\\n{} points on entry side left after removing entry_counts outlier points'.format(clean_df_entries.shape[0]))\n",
    "\n",
    "#DataFrame with exit outliers removed\n",
    "clean_df_exits = df.loc[~df['exit_outliers']]\n",
    "print('{} points on exit side left left after removing exit_counts outlier points'.format(clean_df_exits.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335147 points left after removing entry_counts Nan values\n",
      "2316981 points left after removing entry_counts Nan values\n"
     ]
    }
   ],
   "source": [
    "# Eliminate Null values\n",
    "# Delete rows with null values for entry_counts\n",
    "clean_df_entries = clean_df_entries[~clean_df_entries.entry_counts.isnull()]\n",
    "print('{} points left after removing entry_counts Nan values'.format(clean_df_entries.shape[0]))\n",
    "\n",
    "# Delete rows with null values for exit_counts\n",
    "clean_df_entries = clean_df_entries[~clean_df_entries.exit_counts.isnull()]\n",
    "print('{} points left after removing entry_counts Nan values'.format(clean_df_exits.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames\n",
    "clean_df = clean_df_entries.merge(clean_df_exits,left_on=list(clean_df_entries.columns), \\\n",
    "                                  right_on=list(clean_df_exits.columns),how='inner')\n",
    "#Add a column for total traffic at each turnstile\n",
    "clean_df['total_traffic'] = clean_df['entry_counts'] + clean_df['exit_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2284157 rows left after cleaning the data, 94.80504872744177% of the original\n"
     ]
    }
   ],
   "source": [
    "print('{} rows left after cleaning the data, {}% of the original'.format(clean_df.shape[0],100*(clean_df.shape[0]/df.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete outlier ID columns\n",
    "# Only run this cell once\n",
    "del clean_df['entry_outliers']\n",
    "del clean_df['exit_outliers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data to access easily later\n",
    "with open('data/clean_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(clean_df, to_write)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.5: Read in the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the pickle file if you are starting here\n",
    "# If you started from the beginning of the notebook, comment out the code below\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('data/clean_df.pickle','rb') as read_file:\n",
    "    clean_df = pickle.load(read_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
